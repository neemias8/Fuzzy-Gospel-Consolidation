#!/usr/bin/env python3
"""
Test Evaluation Pipeline

Tests the evaluation functionality including ROUGE, METEOR, BERTScore, and Kendall's Tau.
"""

import sys
import logging
from pathlib import Path

# Add src to path
sys.path.append(str(Path(__file__).parent / 'src'))

from main import FuzzyGospelConsolidator

def test_evaluation_pipeline():
    """Test the complete pipeline with evaluation"""
    print("üß™ TESTING EVALUATION PIPELINE")
    print("=" * 50)
    
    try:
        # Initialize consolidator
        consolidator = FuzzyGospelConsolidator()
        
        # Run steps 1-5 (data loading through summary generation)
        print("\n1Ô∏è‚É£ Running pipeline steps 1-5...")
        consolidator.load_data()
        consolidator.calculate_fuzzy_relations()
        consolidator.build_graph()
        consolidator.train_model()
        summary = consolidator.generate_summary()
        
        print(f"   ‚úÖ Generated summary: {len(summary)} characters")
        
        # Step 6: Evaluation
        print("\n2Ô∏è‚É£ Step 6: Running comprehensive evaluation...")
        evaluation_results = consolidator.evaluate_results()
        
        print("   ‚úÖ Evaluation completed successfully!")
        
        # Display key metrics
        print("\nüìä EVALUATION RESULTS:")
        print("-" * 30)
        
        auto_metrics = evaluation_results.get('automatic_metrics', {})
        print(f"ROUGE-1:      {auto_metrics.get('rouge1', 0.0):.4f}")
        print(f"ROUGE-2:      {auto_metrics.get('rouge2', 0.0):.4f}")
        print(f"ROUGE-L:      {auto_metrics.get('rougeL', 0.0):.4f}")
        print(f"BERTScore F1: {auto_metrics.get('bertscore_f1', 0.0):.4f}")
        print(f"METEOR:       {auto_metrics.get('meteor', 0.0):.4f}")
        print(f"BLEU:         {auto_metrics.get('bleu', 0.0):.4f}")
        
        temporal = evaluation_results.get('temporal_coherence', {})
        print(f"Kendall's Tau: {temporal.get('kendall_tau', 0.0):.4f}")
        print(f"Temporal Acc:  {temporal.get('temporal_accuracy', 0.0):.4f}")
        print(f"Violations:    {temporal.get('chronological_violations', 0)}")
        
        print(f"Overall Score: {evaluation_results.get('overall_score', 0.0):.4f}")
        
        # Test individual metrics
        print("\n3Ô∏è‚É£ Testing individual metrics...")
        from evaluation.metrics import AutomaticMetrics
        
        metrics = AutomaticMetrics()
        
        # Test ROUGE
        sample_text = "Jesus went to Jerusalem and taught in the temple."
        references = ["Jesus traveled to Jerusalem and preached in the temple courts."]
        rouge_scores = metrics.calculate_rouge(sample_text, references)
        print(f"   ‚úÖ ROUGE scores: {rouge_scores}")
        
        # Test BERTScore
        bert_scores = metrics.calculate_bertscore(sample_text, references)
        print(f"   ‚úÖ BERTScore: {bert_scores}")
        
        # Test METEOR
        meteor_score = metrics.calculate_meteor(sample_text, references)
        print(f"   ‚úÖ METEOR: {meteor_score:.4f}")
        
        # Test Kendall's Tau
        generated_order = ["event1", "event2", "event3"]
        reference_order = ["event1", "event3", "event2"]
        tau = metrics.calculate_kendall_tau(generated_order, reference_order)
        print(f"   ‚úÖ Kendall's Tau: {tau:.4f}")
        
        print("\n4Ô∏è‚É£ Checking saved files...")
        from pathlib import Path
        results_dir = Path("results")
        
        if results_dir.exists():
            json_files = list(results_dir.glob("evaluation_results_*.json"))
            txt_files = list(results_dir.glob("consolidated_summary_*.txt"))
            report_files = list(results_dir.glob("evaluation_report_*.txt"))
            
            print(f"   ‚úÖ Found {len(json_files)} evaluation JSON files")
            print(f"   ‚úÖ Found {len(txt_files)} summary text files")
            print(f"   ‚úÖ Found {len(report_files)} evaluation report files")
            
            if report_files:
                latest_report = max(report_files, key=lambda f: f.stat().st_mtime)
                print(f"   üìÑ Latest report: {latest_report.name}")
        
        print("\nüéâ ALL EVALUATION TESTS PASSED!")
        print("   ‚úÖ ROUGE, METEOR, BERTScore calculations working")
        print("   ‚úÖ Kendall's Tau temporal ordering evaluation working")
        print("   ‚úÖ Results saved to files successfully")
        print("   ‚úÖ Complete 6-step pipeline functional")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå EVALUATION TEST FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(level=logging.INFO)
    
    # Run test
    success = test_evaluation_pipeline()
    
    if success:
        print("\n" + "=" * 60)
        print("üéØ COMPLETE PIPELINE VERIFIED:")
        print("   1. Text extraction from Gospel events ‚úÖ")
        print("   2. Fuzzy relations (Œº_same, Œº_conflict, Œº_before) ‚úÖ")
        print("   3. Fuzzy-GNN graph construction and processing ‚úÖ")
        print("   4. Consolidated summary generation ‚úÖ")
        print("   5. Comprehensive evaluation with metrics ‚úÖ")
        print("   6. Results saved to files ‚úÖ")
        sys.exit(0)
    else:
        sys.exit(1)